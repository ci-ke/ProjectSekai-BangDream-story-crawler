# deepseek

import os
import re
from bs4 import BeautifulSoup
import glob


def extract_event_info(html_file_path):
    """ä»HTMLæ–‡ä»¶ä¸­æå–æ´»åŠ¨ä¿¡æ¯"""
    with open(html_file_path, 'r', encoding='utf-8') as f:
        html_content = f.read()

    soup = BeautifulSoup(html_content, 'html.parser')

    # æå–æ´»åŠ¨ID - ä»å¤šä¸ªå¯èƒ½çš„ä½ç½®æŸ¥æ‰¾
    event_id = None
    # æ–¹æ³•1: ä»åŒ…å«"events/"çš„é“¾æ¥ä¸­æå–
    links = soup.find_all('a', href=True)
    for link in links:
        href = link['href']
        # æŸ¥æ‰¾ç±»ä¼¼"events/190"æˆ–"/events/190"çš„é“¾æ¥
        match = re.search(r'(?:/|^)events/(\d+)', href)  # type: ignore
        if match:
            event_id = match.group(1)
            break

        # æ–¹æ³•2: ä»åŒ…å«"eventstory/"çš„é“¾æ¥ä¸­æå–
        match = re.search(r'(?:/|^)eventstory/(\d+)', href)  # type: ignore
        if match:
            event_id = match.group(1)
            break

    # å¦‚æœè¿˜æ²¡æ‰¾åˆ°ï¼Œå°è¯•ä»å½“å‰é¡µé¢çš„URLä¸­æå–
    if not event_id and 'eventstory/' in html_file_path:
        match = re.search(r'eventstory/(\d+)', html_file_path)
        if match:
            event_id = match.group(1)

    # æå–æ´»åŠ¨æ ‡é¢˜
    chinese_title = ""
    japanese_title = ""

    h1_tag = soup.find('h1', class_=re.compile('text-.*'))
    if h1_tag:
        chinese_title = h1_tag.get_text(strip=True)

    # æ—¥æ–‡æ ‡é¢˜é€šå¸¸åœ¨ä¸­æ–‡æ ‡é¢˜ä¸‹é¢çš„<p>æ ‡ç­¾ä¸­
    if h1_tag:
        next_p = h1_tag.find_next('p', class_=re.compile('text-.*'))
        if next_p:
            japanese_title = next_p.get_text(strip=True)

    # æå–èƒŒæ™¯æè¦
    background = ""
    # æŸ¥æ‰¾åŒ…å«"èƒŒæ™¯æè¦"çš„h3æ ‡ç­¾
    h3_tags = soup.find_all('h3')
    for h3 in h3_tags:
        if 'èƒŒæ™¯æè¦' in h3.get_text():
            next_p = h3.find_next('p')
            if next_p:
                background = next_p.get_text(strip=True)
                break

    # æå–æ´»åŠ¨æ¦‚è¦
    summary = ""
    # æŸ¥æ‰¾åŒ…å«"æ´»åŠ¨æ¦‚è¦"çš„h2æ ‡ç­¾
    h2_tags = soup.find_all('h2')
    for h2 in h2_tags:
        if 'æ´»åŠ¨æ¦‚è¦' in h2.get_text():
            # æ¦‚è¦é€šå¸¸åœ¨h2åé¢çš„divæˆ–pæ ‡ç­¾ä¸­
            next_div = h2.find_next('div', class_=re.compile('prose.*'))
            if next_div:
                summary = next_div.get_text(strip=True)
                break

    # æå–ç« èŠ‚åˆ—è¡¨
    chapters = []
    # æŸ¥æ‰¾æ‰€æœ‰åŒ…å«ç« èŠ‚çš„<a>æ ‡ç­¾
    chapter_links = soup.find_all('a', href=re.compile(r'eventstory/\d+/\d+/'))
    for link in chapter_links:
        # æå–ç« èŠ‚ç¼–å·
        href = link['href']
        chapter_match = re.search(r'eventstory/\d+/(\d+)/', href)  # type: ignore
        chapter_num = chapter_match.group(1) if chapter_match else "0"

        # æå–ç« èŠ‚æ ‡é¢˜
        title = ""
        title_tag = link.find('h3')
        if title_tag:
            title = title_tag.get_text(strip=True)

        # æå–ç« èŠ‚æè¿°
        description = ""
        desc_tag = link.find('p', class_=re.compile('text-sm.*'))
        if desc_tag:
            description = desc_tag.get_text(strip=True)

        # æ¸…ç†æè¿°æ–‡æœ¬ï¼Œå»é™¤å¤šä½™çš„ç©ºç™½
        description = ' '.join(description.split())

        chapters.append(
            {'number': chapter_num, 'title': title, 'description': description}
        )

    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç« èŠ‚é“¾æ¥ï¼Œå°è¯•å¦ä¸€ç§æŸ¥æ‰¾æ–¹å¼
    if not chapters:
        # æŸ¥æ‰¾æ‰€æœ‰åŒ…å«ç« èŠ‚ç¼–å·çš„div
        chapter_divs = soup.find_all(
            'div',
            class_=lambda x: x and 'absolute' in x and 'top-1' in x and 'left-1' in x,  # type: ignore
        )
        for div in chapter_divs:
            if '#' in div.get_text():
                chapter_num = div.get_text(strip=True).replace('#', '')

                # æŸ¥æ‰¾åŒä¸€çˆ¶å®¹å™¨ä¸­çš„æ ‡é¢˜å’Œæè¿°
                parent_div = div.find_parent('div', class_=re.compile('bg-white.*'))
                if parent_div:
                    title_tag = parent_div.find('h3')
                    title = title_tag.get_text(strip=True) if title_tag else ""

                    desc_tag = parent_div.find('p', class_=re.compile('text-sm.*'))
                    description = desc_tag.get_text(strip=True) if desc_tag else ""
                    description = ' '.join(description.split())

                    chapters.append(
                        {
                            'number': chapter_num,
                            'title': title,
                            'description': description,
                        }
                    )

    # æŒ‰ç« èŠ‚ç¼–å·æ’åº
    chapters.sort(key=lambda x: int(x['number']))

    return {
        'event_id': event_id,
        'chinese_title': chinese_title,
        'japanese_title': japanese_title,
        'background': background,
        'summary': summary,
        'chapters': chapters,
    }


def generate_txt_content(event_info):
    """ç”ŸæˆTXTæ–‡ä»¶å†…å®¹"""
    content = f"æ´»åŠ¨æ ‡é¢˜: {event_info['chinese_title']}\n"
    content += f"æ—¥æ–‡æ ‡é¢˜: {event_info['japanese_title']}\n"
    content += f"æ´»åŠ¨ID: #{event_info['event_id']}\n"
    content += "=" * 50 + "\n\n"

    if event_info['background']:
        content += "ğŸ“‹ èƒŒæ™¯æè¦\n"
        content += f"{event_info['background']}\n\n"

    if event_info['summary']:
        content += "ğŸ“ æ´»åŠ¨æ¦‚è¦\n"
        content += f"{event_info['summary']}\n\n"

    content += "=" * 50 + "\n\n"

    if event_info['chapters']:
        content += "ğŸ“– ç« èŠ‚åˆ—è¡¨\n\n"
        for chapter in event_info['chapters']:
            content += f"ç¬¬{chapter['number']}ç« : {chapter['title']}\n"
            content += f"{chapter['description']}\n"
            content += "-" * 30 + "\n\n"

    return content


def process_html_files(input_folder, output_folder):
    """å¤„ç†HTMLæ–‡ä»¶å¹¶ç”ŸæˆTXTæ–‡ä»¶"""
    # ç¡®ä¿è¾“å‡ºæ–‡ä»¶å¤¹å­˜åœ¨
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)

    # æŸ¥æ‰¾æ‰€æœ‰HTMLæ–‡ä»¶
    html_files = glob.glob(os.path.join(input_folder, "*.html"))

    for html_file in html_files:
        print(f"å¤„ç†æ–‡ä»¶: {html_file}")

        try:
            # æå–ä¿¡æ¯
            event_info = extract_event_info(html_file)

            if not event_info['event_id']:
                print(f"  è­¦å‘Š: æ— æ³•ä» {html_file} ä¸­æå–æ´»åŠ¨IDï¼Œè·³è¿‡")
                continue

            # ç”ŸæˆTXTæ–‡ä»¶å
            # æ¸…ç†æ ‡é¢˜ä¸­çš„éæ³•æ–‡ä»¶åå­—ç¬¦
            safe_title = re.sub(r'[<>:"/\\|?*]', '', event_info['chinese_title'])
            if not safe_title or safe_title.isspace():
                safe_title = f"event_{event_info['event_id']}"

            txt_filename = f"event_{event_info['event_id']}_{safe_title}.txt"
            txt_path = os.path.join(output_folder, txt_filename)

            # ç”Ÿæˆå†…å®¹å¹¶ä¿å­˜
            txt_content = generate_txt_content(event_info)

            with open(txt_path, 'w', encoding='utf-8') as f:
                f.write(txt_content)

            print(f"  å·²ç”Ÿæˆ: {txt_path}")

        except Exception as e:
            print(f"  å¤„ç† {html_file} æ—¶å‡ºé”™: {str(e)}")


def main():
    # è®¾ç½®è¾“å…¥å’Œè¾“å‡ºæ–‡ä»¶å¤¹
    input_folder = "html_pages"
    output_folder = "txt_output"

    # å¤„ç†æ‰€æœ‰HTMLæ–‡ä»¶
    process_html_files(input_folder, output_folder)

    print("\nå¤„ç†å®Œæˆï¼")


if __name__ == "__main__":
    main()
